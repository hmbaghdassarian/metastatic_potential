{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7df580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "import numpy as np\n",
    "\n",
    "def iterative_elastic_net(X_train, y_train, num_iterations=100, selection_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Perform iterative ElasticNetCV and retain features selected in a majority of iterations.\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (np.array): Training feature matrix.\n",
    "        y_train (np.array): Training target vector.\n",
    "        num_iterations (int): Number of ElasticNetCV iterations.\n",
    "        selection_threshold (float): Minimum fraction of iterations a feature must be selected.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Mask of selected features.\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "    feature_selection_counts = np.zeros(n_features)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Set a random seed for reproducibility\n",
    "        random_seed = np.random.randint(0, 10000)\n",
    "        elastic_net = ElasticNetCV(cv=5, random_state=random_seed)\n",
    "        elastic_net.fit(X_train, y_train)\n",
    "        \n",
    "        # Record features with non-zero coefficients\n",
    "        feature_selection_counts += (elastic_net.coef_ != 0).astype(int)\n",
    "\n",
    "    # Calculate the frequency of selection\n",
    "    selection_frequency = feature_selection_counts / num_iterations\n",
    "    \n",
    "    # Retain features selected above the threshold\n",
    "    selected_features = selection_frequency > selection_threshold\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def elastic_net_iteration(args):\n",
    "    \"\"\"\n",
    "    Perform a single iteration of ElasticNetCV for feature selection.\n",
    "\n",
    "    Parameters:\n",
    "        args (tuple): A tuple containing (X_train, y_train, random_seed).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Binary mask indicating selected features.\n",
    "    \"\"\"\n",
    "    X_train, y_train, random_seed = args\n",
    "    elastic_net = ElasticNetCV(cv=5, random_state=random_seed)\n",
    "    elastic_net.fit(X_train, y_train)\n",
    "    return (elastic_net.coef_ != 0).astype(int)\n",
    "\n",
    "def iterative_elastic_net_parallel(X_train, y_train, num_iterations=100, selection_threshold=0.8, n_jobs=None):\n",
    "    \"\"\"\n",
    "    Perform iterative ElasticNetCV in parallel and retain features selected in a majority of iterations.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (np.array): Training feature matrix.\n",
    "        y_train (np.array): Training target vector.\n",
    "        num_iterations (int): Number of ElasticNetCV iterations.\n",
    "        selection_threshold (float): Minimum fraction of iterations a feature must be selected.\n",
    "        n_jobs (int): Number of parallel processes (default is None, which uses all available cores).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Mask of selected features.\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]\n",
    "\n",
    "    # Create random seeds for each iteration\n",
    "    random_seeds = np.random.randint(0, 10000, size=num_iterations)\n",
    "\n",
    "    # Prepare arguments for parallel processing\n",
    "    args = [(X_train, y_train, seed) for seed in random_seeds]\n",
    "\n",
    "    # Perform iterations in parallel\n",
    "    with Pool(processes=n_jobs) as pool:\n",
    "        feature_selection_masks = pool.map(elastic_net_iteration, args)\n",
    "\n",
    "    # Calculate the frequency of feature selection\n",
    "    feature_selection_counts = np.sum(feature_selection_masks, axis=0)\n",
    "    selection_frequency = feature_selection_counts / num_iterations\n",
    "\n",
    "    # Retain features selected above the threshold\n",
    "    selected_features = selection_frequency > selection_threshold\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ed5f981",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15937e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "data_path = '/nobackup/users/hmbaghda/metastatic_potential/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "405ae92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651b4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmp.to_csv(os.path.join(data_path, 'processed', 'metastatic_potential.csv'))\n",
    "nexpr.to_csv(os.path.join(data_path, 'processed', 'expr.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62989717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56d69f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c87741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301597c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976d73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Outer CV loop\n",
    "n_splits = 10\n",
    "outer_cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "outer_results = []\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X):\n",
    "    X_outer_train, X_outer_test = X[train_idx], X[test_idx]\n",
    "    y_outer_train, y_outer_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Inner CV loop\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    inner_selected_features = []\n",
    "    inner_best_params = []\n",
    "\n",
    "    for inner_train_idx, inner_val_idx in inner_cv.split(X_outer_train):\n",
    "        X_inner_train, X_inner_val = X_outer_train[inner_train_idx], X_outer_train[inner_val_idx]\n",
    "        y_inner_train, y_inner_val = y_outer_train[inner_train_idx], y_outer_train[inner_val_idx]\n",
    "\n",
    "        # Step 1: Feature selection with ElasticNetCV -- replace with calling R, or see chatgpt for python version\n",
    "        # iterate many times\n",
    "        elastic_net = ElasticNetCV(cv=n_splits, random_state=42)\n",
    "        elastic_net.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "        # Identify selected features (non-zero coefficients)\n",
    "#         selected_features = np.where(elastic_net.coef_ != 0)[0]\n",
    "#         selected_features = iterative_elastic_net(X_inner_train, y_inner_train, num_iterations=100, selection_threshold=0.8)\n",
    "        selected_features_mask = iterative_elastic_net_parallel(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            num_iterations=100,\n",
    "            selection_threshold=0.8,\n",
    "            n_jobs=4  # Use 4 parallel processes\n",
    "    )\n",
    "        X_inner_train_reduced = X_inner_train[:, selected_features]\n",
    "        X_inner_val_reduced = X_inner_val[:, selected_features]\n",
    "\n",
    "        inner_selected_features.append(selected_features)\n",
    "\n",
    "        # Step 2: Hyperparameter tuning (e.g., Random Forest on reduced feature set)\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10],\n",
    "            'min_samples_split': [2, 5],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "\n",
    "        best_params = None\n",
    "        best_score = float('inf')\n",
    "\n",
    "        for n_estimators, max_depth, min_samples_split, min_samples_leaf in itertools.product(\n",
    "                param_grid['n_estimators'], param_grid['max_depth'],\n",
    "                param_grid['min_samples_split'], param_grid['min_samples_leaf']):\n",
    "            \n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                min_samples_split=min_samples_split,\n",
    "                min_samples_leaf=min_samples_leaf,\n",
    "                random_state=42\n",
    "            )\n",
    "            model.fit(X_inner_train_reduced, y_inner_train)\n",
    "            val_predictions = model.predict(X_inner_val_reduced)\n",
    "            val_score = mean_squared_error(y_inner_val, val_predictions)\n",
    "\n",
    "            if val_score < best_score:\n",
    "                best_score = val_score\n",
    "                best_params = {\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf\n",
    "                }\n",
    "\n",
    "        inner_best_params.append(best_params)\n",
    "\n",
    "    # Aggregate selected features and hyperparameters across inner folds\n",
    "    feature_counts = Counter([tuple(features) for features in inner_selected_features])\n",
    "    consensus_features = np.array(feature_counts.most_common(1)[0][0])\n",
    "\n",
    "    param_counts = Counter(tuple(params.items()) for params in inner_best_params)\n",
    "    consensus_params = dict(param_counts.most_common(1)[0][0])\n",
    "\n",
    "    # Step 3: Train final model on outer training set with consensus features\n",
    "    X_outer_train_reduced = X_outer_train[:, consensus_features]\n",
    "    X_outer_test_reduced = X_outer_test[:, consensus_features]\n",
    "\n",
    "    final_model = RandomForestRegressor(**consensus_params, random_state=42)\n",
    "    final_model.fit(X_outer_train_reduced, y_outer_train)\n",
    "\n",
    "    # Evaluate on outer test set\n",
    "    test_predictions = final_model.predict(X_outer_test_reduced)\n",
    "    test_score = mean_squared_error(y_outer_test, test_predictions)\n",
    "    test_correlation = np.corrcoef(test_predictions, y_outer_test)[0, 1]\n",
    "    \n",
    "    \n",
    "    # add linear and random baselines here\n",
    "    \n",
    "    outer_results.append((test_score, test_correlation))\n",
    "\n",
    "# Report results\n",
    "print(\"Outer Fold Results (MSE, Pearson Correlation):\", outer_results)\n",
    "print(\"Average MSE:\", np.mean([result[0] for result in outer_results]))\n",
    "print(\"Average Pearson Correlation:\", np.mean([result[1] for result in outer_results]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92af9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Assume `all_selected_features` and `all_best_params` are from the outer loop\n",
    "# Step 1: Aggregate consensus features\n",
    "feature_counts = Counter([tuple(features) for features in all_selected_features])\n",
    "final_features = np.array(feature_counts.most_common(1)[0][0])\n",
    "\n",
    "# Step 2: Aggregate consensus hyperparameters\n",
    "param_counts = Counter(tuple(params.items()) for params in all_best_params)\n",
    "final_params = dict(param_counts.most_common(1)[0][0])\n",
    "\n",
    "print(\"Final Selected Features:\", final_features)\n",
    "print(\"Final Hyperparameters:\", final_params)\n",
    "\n",
    "# Step 3: Train final model on the entire dataset\n",
    "X_reduced = X[:, final_features]  # Use the entire dataset with final features\n",
    "final_model = RandomForestRegressor(**final_params, random_state=42)\n",
    "final_model.fit(X_reduced, y)  # Train on all data\n",
    "\n",
    "# Step 4: Save the final model (optional)\n",
    "import joblib\n",
    "joblib.dump(final_model, \"final_random_forest_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27299a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4767d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547355c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mp_2]",
   "language": "python",
   "name": "conda-env-mp_2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
