{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73a212c",
   "metadata": {},
   "source": [
    "Next, we run the data on many different models to get a set of candidate models that seem to have high performance. We do a preliminary hyperparameter search of the likely most influential parameters at coarse resolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b0d1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import argparse\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.linear_model import Lasso,Ridge,ElasticNet\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as XGB\n",
    "\n",
    "import plotnine as p9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35484b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17879"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63de970",
   "metadata": {},
   "source": [
    "Specify parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeca60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Run different ML models')\n",
    "# parser.add_argument('--model_types', metavar='N', type=str, nargs='*', help='models to train',default=['PLSR','knn' ,'svmLinear','svmRBF','svmPoly','lasso','ridge','elasticNet','neuralNet','xgboost','rf'])\n",
    "# parser.add_argument('--X_path', action='store', type=str,help='path to the input training data',default='X.csv')\n",
    "# parser.add_argument('--Y_path', action='store', type=str,help='path to the output training data',default='y.csv')\n",
    "# parser.add_argument('--num_folds', action='store', type=int,help='number of folds',default=10)\n",
    "# parser.add_argument('--res_dir', action='store', type=str,help='Results directory',default='/nobackup/users/hmbaghda/metastatic_potential/interim/')\n",
    "# parser.add_argument('--seed', action='store', type=int,help='seed',default=42)\n",
    "# parser.add_argument('--grid_search', action='store', type=bool,help='Nested Hyperparameter Tuning',default=True)\n",
    "# parser.add_argument('--hyperparam_folds', action='store', type=int,help='number of hyperparametr tuning folds',default=5)\n",
    "# parser.add_argument('--n_cores', action='store', type=int,help='number of cores for parallelization',default=20)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# model_types = args.model_types\n",
    "# num_folds = args.num_folds\n",
    "# X_path = args.X_path\n",
    "# Y_path = args.Y_path\n",
    "# res_dir= args.res_dir\n",
    "# seed = args.seed\n",
    "# cv_folds = args.hyperparam_folds\n",
    "\n",
    "# params\n",
    "data_path = '/nobackup/users/hmbaghda/metastatic_potential/'\n",
    "res_dir = os.path.join(data_path, 'interim')\n",
    "\n",
    "hvg_selection = False # whether to filter for HVGs\n",
    "hvg_name = '' if not hvg_selection else 'hvg_'\n",
    "X_path = os.path.join(data_path, 'processed',  hvg_name + 'expr.csv')\n",
    "Y_path = os.path.join(data_path, 'processed', 'metastatic_potential.csv')\n",
    "\n",
    "seed = 42\n",
    "\n",
    "num_folds = 10\n",
    "grid_search = True\n",
    "cv_folds = 5\n",
    "n_cores = 30\n",
    "\n",
    "model_types = ['PLSR','elasticNet', 'svm', \n",
    "               'rf', 'xgboost', 'knn']\n",
    "model_types = ['PLSR', 'ridge']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aedabac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ba5e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(X_path,index_col=0).T\n",
    "Y = pd.read_csv(Y_path,index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8becc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_r(y_true, y_pred):\n",
    "    x = y_true\n",
    "    y = y_pred\n",
    "    mx = torch.mean(x, dim=0)\n",
    "    my = torch.mean(y, dim=0)\n",
    "    xm, ym = x - mx, y - my\n",
    "    r_num = torch.sum(xm * ym,dim=0)\n",
    "    x_square_sum = torch.sum(xm * xm,dim=0)\n",
    "    y_square_sum = torch.sum(ym * ym,dim=0)\n",
    "    r_den = torch.sqrt(x_square_sum * y_square_sum)\n",
    "    r = r_num / r_den\n",
    "    return r #torch.mean(r)\n",
    "\n",
    "def pair_pearsonr(x, y, axis=0):\n",
    "    mx = np.mean(x, axis=axis, keepdims=True)\n",
    "    my = np.mean(y, axis=axis, keepdims=True)\n",
    "    xm, ym = x-mx, y-my\n",
    "    r_num = np.add.reduce(xm * ym, axis=axis)\n",
    "    r_den = np.sqrt((xm*xm).sum(axis=axis) * (ym*ym).sum(axis=axis))\n",
    "    r = r_num / r_den\n",
    "    return r\n",
    "\n",
    "def getSamples(N, batchSize):\n",
    "    order = np.random.permutation(N)\n",
    "    outList = []\n",
    "    while len(order)>0:\n",
    "        outList.append(order[0:batchSize])\n",
    "        order = order[batchSize:]\n",
    "    return outList\n",
    "\n",
    "def L2Regularization(deepLearningModel, L2):\n",
    "    weightLoss = 0.\n",
    "    biasLoss = 0.\n",
    "    for layer in deepLearningModel:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            weightLoss = weightLoss + L2 * torch.sum((layer.weight)**2)\n",
    "            biasLoss = biasLoss + L2 * torch.sum((layer.bias)**2)\n",
    "    L2Loss = biasLoss + weightLoss\n",
    "    return(L2Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5c75e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001, 0.001, 0.01, 0.1, 1, 10, 100]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[10**i for i in range(-4, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "374f3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = range(100, 1001, 250)\n",
    "svm_c = [10**i for i in range(-3, 3)]\n",
    "svm_gamma = [10**i for i in range(-3, 2)]\n",
    "# svm_epsilon = [10**i for i in range(-2, 1)]\n",
    "alpha = [10**i for i in range(-3, 3)]\n",
    "\n",
    "grid_search_params = {\n",
    "    'knn': {\n",
    "        'n_neighbors': range(5, 41, 5)\n",
    "    },\n",
    "    'plsr': {\n",
    "        'n_components': [15, 25, 50, 100]#range(2, 16, 2)\n",
    "    }, \n",
    "    'rf': {\n",
    "        'n_estimators': n_estimators\n",
    "            },\n",
    "    'xgboost': {\n",
    "        'n_estimators': n_estimators\n",
    "    },\n",
    "    'svm': {\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'C': svm_c, \n",
    "        'gamma': svm_gamma,  # only poly and rbf\n",
    "        'degree': [2,3,4,5],  # only for poly\n",
    "#         'coef0':[0,0.1,0.5,1.,1.2,2.] # only for poly, \n",
    "        \n",
    "    },\n",
    "#     'svmRBF': {\n",
    "#         'C': svm_c,\n",
    "#         'gamma': svm_gamma,\n",
    "#         'epsilon': svm_epsilon\n",
    "#     }, \n",
    "#     'svmPoly':{\n",
    "#         'gamma': svm_gamma,\n",
    "#         'C': svm_c,\n",
    "#         'degree':[2,3,4,5],\n",
    "#         'coef0':[0,0.1,0.5,1.,1.2,2.]\n",
    "#     }, \n",
    "#     'lasso':{\n",
    "#         'alpha': alpha\n",
    "#     }, \n",
    "    'ridge': {\n",
    "        'alpha': alpha,\n",
    "    }, \n",
    "    'elasticNet': {\n",
    "        'alpha': alpha,\n",
    "        'l1_ratio': np.arange(0, 1.01, 0.25) # with 0 and 1 inclusive, this also does ridge and lasso\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b2eb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5131aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09587494",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for mdl in model_types:\n",
    "    if mdl == 'knn':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=KNN(),\n",
    "                                 param_grid = grid_search_params['knn'], \n",
    "                                 cv=cv_folds, \n",
    "                                 n_jobs=n_cores)\n",
    "        else:\n",
    "            model = KNN(n_neighbors=5) # default value\n",
    "    elif mdl=='PLSR':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=PLSRegression(scale=False),\n",
    "                                 param_grid = grid_search_params['plsr'], cv=cv_folds, n_jobs=n_cores)\n",
    "        else:\n",
    "            model = PLSRegression(n_components=4,scale=False)\n",
    "    elif mdl == 'rf':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=RandomForestRegressor(),\n",
    "                                 param_grid = grid_search_params['rf'], cv=cv_folds, n_jobs=n_cores)\n",
    "        else:\n",
    "            model = RandomForestRegressor(n_estimators=800, n_jobs = -1)\n",
    "    elif mdl == 'xgboost':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=XGB.XGBRegressor(),\n",
    "                                 param_grid = grid_search_params['xgboost'], cv=cv_folds, n_jobs=n_cores)\n",
    "        else:\n",
    "            model = XGB.XGBRegressor(n_estimators=800,n_jobs = -1)\n",
    "    elif mdl == 'svm':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=SVR(),\n",
    "                                 param_grid = grid_search_params['svm'], cv=cv_folds, n_jobs=n_cores)\n",
    "        else:\n",
    "            model = SVR(kernel='rbf')\n",
    "#     elif mdl == 'svmLinear':\n",
    "#         if grid_search:\n",
    "#             model = GridSearchCV(estimator=LinearSVR(),\n",
    "#                                  param_grid = grid_search_params['svmLinear'], cv=cv_folds, n_jobs=n_cores)\n",
    "#         else:\n",
    "#             model = LinearSVR()\n",
    "#     elif mdl == 'svmRBF':\n",
    "#         if grid_search:\n",
    "#             model = GridSearchCV(estimator=SVR(kernel='rbf'),\n",
    "#                                  param_grid = grid_search_params['svmRBF'], \n",
    "#                                  cv=cv_folds, n_jobs=n_cores)\n",
    "#         else:\n",
    "#             model = SVR(kernel='rbf')\n",
    "#     elif mdl == 'svmPoly':\n",
    "#         if grid_search:\n",
    "#             model = GridSearchCV(estimator=SVR(kernel='poly'),\n",
    "#                                  param_grid = grid_search_params['svmPoly'], \n",
    "#                                  cv=cv_folds, n_jobs=n_cores)\n",
    "#         else:\n",
    "#             model = SVR(kernel='poly')\n",
    "#     elif mdl == 'lasso':\n",
    "#         if grid_search:\n",
    "#             model = GridSearchCV(estimator=Lasso(),\n",
    "#                                  param_grid = grid_search_params['lasso'], cv=cv_folds, n_jobs=n_cores)\n",
    "#         else:\n",
    "#             model = Lasso(alpha=0.1)\n",
    "#     elif mdl == 'ridge':\n",
    "#         if grid_search:\n",
    "#             model = GridSearchCV(estimator=Ridge(),param_grid = grid_search_params['ridge'], \n",
    "#                                  cv=cv_folds, n_jobs=n_cores)\n",
    "#         else:\n",
    "#             model = Ridge(alpha=0.1)\n",
    "    elif mdl == 'elasticNet':\n",
    "        if grid_search:\n",
    "            model = GridSearchCV(estimator=ElasticNet(),\n",
    "                                 param_grid = grid_search_params['elasticNet'], cv=cv_folds, n_jobs=n_cores)\n",
    "        else:\n",
    "            model = ElasticNet(alpha=0.1)\n",
    "#     elif mdl == 'neuralNet':\n",
    "#         device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#         model = 'define the ANN just before it is trained'\n",
    "#         epochs = 100\n",
    "#         l2_reg  = 0.01\n",
    "#         bs = 20\n",
    "#         criterion = torch.nn.MSELoss(reduction='mean')\n",
    "#     models.append(model)\n",
    "    models[mdl] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b14ea",
   "metadata": {},
   "source": [
    "Run the iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b012e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_augmentation(x, y,\n",
    "                       seed:int=42,\n",
    "                       n_synthetic: int = 1000, \n",
    "                      alpha: float = 1):\n",
    "    \"\"\"Mixup augmentation strategy\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy array\n",
    "        input X block\n",
    "    y : numpy array\n",
    "        inpute y block\n",
    "    seed : int, optional\n",
    "        _description_, by default 42\n",
    "    n_synthetic : int, optional\n",
    "        number of synthetic data points to make, by default 1000\n",
    "    alpha : float, optional\n",
    "        controls the parameter for generating the mixup coefficient, by default 1 (which will draw lambda uniformly from 0 to 1)\n",
    "        must be a positive number. The larger the number, the more the synthetic data deviates from the original\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples, n_features = x.shape\n",
    "    synthetic_x = np.zeros((n_synthetic, n_features))\n",
    "    synthetic_y = np.zeros((n_synthetic, ))\n",
    "    for i in range(n_synthetic):\n",
    "        idx1, idx2 = np.random.choice(n_samples, size=2, replace=False)\n",
    "        lambda_ = np.random.beta(alpha, alpha) # alpha = beta means drawn symmetrically about 0.5\n",
    "        synthetic_x[i] = lambda_ * x[idx1] + (1 - lambda_) * x[idx2]\n",
    "        synthetic_y[i] = lambda_ * y[idx1] + (1 - lambda_) * y[idx2]\n",
    "        \n",
    "    x_all = np.vstack([x, synthetic_x])\n",
    "    y_all = np.concatenate([y, synthetic_y])\n",
    "    return x_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2619f2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begun fitting and evaluation for model: PLSR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/hmbaghda/Software/miniforge3/envs/metastatic_potential/lib/python3.13/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=num_folds,shuffle=True,random_state=seed)\n",
    "\n",
    "res = pd.DataFrame(columns = ['model_type', 'fold', 'train_pearson', 'test_pearson', 'best_params'])\n",
    "for model_type, model in tqdm(models.items()):\n",
    "    print('Begun fitting and evaluation for model: %s'%model_type)\n",
    "    for k, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "        x_train = X.iloc[train_index,:].values\n",
    "        x_test = X.iloc[test_index,:].values\n",
    "        y_train = Y.iloc[train_index,:].values.ravel()\n",
    "        y_test = Y.iloc[test_index,:].values.ravel()\n",
    "\n",
    "        x_train, y_train = mixup_augmentation(x = x_train, \n",
    "                                              y = y_train,\n",
    "                                     seed = seed + k, \n",
    "                                     n_synthetic = 1000, \n",
    "                                     alpha = 1)\n",
    "\n",
    "\n",
    "        # fit model and evaluate in validation set\n",
    "        model.fit(x_train,y_train)\n",
    "        yhat_train = model.predict(x_train)\n",
    "        yhat_test = model.predict(x_test)\n",
    "        \n",
    "        train_pearson=pair_pearsonr(y_train, yhat_train, axis=0).mean()\n",
    "        test_pearson=pair_pearsonr(y_test, yhat_test, axis=0).mean()\n",
    "        \n",
    "        res.loc[res.shape[0], :] = [model_type, k, train_pearson, test_pearson, model.best_params_]\n",
    "        res.to_csv(os.path.join(data_path, 'processed', 'da_coarse_model_tests.csv'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8120929",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metastatic_potential]",
   "language": "python",
   "name": "conda-env-metastatic_potential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
