{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ff9f2b",
   "metadata": {},
   "source": [
    "To do: load in non feature selected, non mean-centered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "dcb1ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import CmaEsSampler, TPESampler, RandomSampler\n",
    "from optuna.distributions import CategoricalDistribution\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7c0dd2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nobackup/users/hmbaghda/metastatic_potential/'\n",
    "random_state = 42\n",
    "\n",
    "n_cores = 80\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0ccf071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pickled_object(object_, file_name: str) -> None:\n",
    "    if '.' in file_name:\n",
    "        p = pathlib.Path(file_name)\n",
    "        extensions = \"\".join(p.suffixes)\n",
    "        file_name = str(p).replace(extensions, '.pickle')\n",
    "    else:\n",
    "        file_name = file_name + '.pickle'\n",
    "\n",
    "    with open(file_name, 'wb') as handle:\n",
    "        pickle.dump(object_, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6133b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection transformer\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, method='top_n_cv', n_features=None):\n",
    "        if method not in ['top_n_cv']:#, 'all_features']:\n",
    "            raise ValueError('Incorrect feature selection method implemented')\n",
    "        self.method = method\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.method == 'top_n_cv':\n",
    "            self.coefficient_of_variation_ = np.std(X, axis=0) / np.mean(X, axis=0)\n",
    "            self.top_indices_ = np.argsort(self.coefficient_of_variation_)[::-1][:self.n_features]\n",
    "#         elif self.method == 'all_features':\n",
    "#             self.top_indices_ = range(X.shape[1])\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X[:, self.top_indices_]\n",
    "    \n",
    "class MeanCenterer(TransformerMixin, BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "#         self.mean_ = np.mean(X, axis=0)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X - np.mean(X, axis=0)\n",
    "    \n",
    "def pearson_corr_scorer(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "class PLSRegression_X(PLSRegression):\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = super().transform(X, y)\n",
    "        if isinstance(X_transformed, tuple):\n",
    "            X_transformed = X_transformed[0]\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d627a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSampler(optuna.samplers.BaseSampler):\n",
    "    def __init__(self, primary_sampler, fallback_sampler):\n",
    "        self.primary_sampler = primary_sampler  # e.g., CmaEsSampler\n",
    "        self.fallback_sampler = fallback_sampler  # e.g., TPESampler\n",
    "\n",
    "    def infer_relative_search_space(self, study, trial):\n",
    "        # Let the primary sampler define the relative search space\n",
    "        return self.primary_sampler.infer_relative_search_space(study, trial)\n",
    "\n",
    "    def sample_relative(self, study, trial, search_space):\n",
    "        # Let the primary sampler handle relative sampling\n",
    "        return self.primary_sampler.sample_relative(study, trial, search_space)\n",
    "\n",
    "    def sample_independent(self, study, trial, param_name, param_distribution):\n",
    "        # Use the fallback sampler for unsupported parameter types\n",
    "        if isinstance(param_distribution, CategoricalDistribution):\n",
    "            return self.fallback_sampler.sample_independent(study, trial, param_name, param_distribution)\n",
    "        # Default to the primary sampler\n",
    "        return self.primary_sampler.sample_independent(study, trial, param_name, param_distribution)\n",
    "\n",
    "class RandomTPESampler(TPESampler):\n",
    "    def __init__(self, exploration_sampler, exploration_freq=20, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.exploration_sampler = exploration_sampler\n",
    "        self.exploration_freq = exploration_freq\n",
    "\n",
    "    def sample_independent(self, study, trial, param_name, param_distribution):\n",
    "        # Use the exploration_sampler periodically\n",
    "        if trial.number % self.exploration_freq == 0:\n",
    "            return self.exploration_sampler.sample_independent(study, trial, param_name, param_distribution)\n",
    "        # Default to TPE\n",
    "        return super().sample_independent(study, trial, param_name, param_distribution)\n",
    "\n",
    "\n",
    "def optuna_objective(trial, X, y, inner_cv, n_cores, random_state):\n",
    "    # Define feature reduction/selection method\n",
    "    feature_step = trial.suggest_categorical(\"feature_step\", [\"PLS\", \"PCA\", \"FeatureSelector\"])\n",
    "\n",
    "    if feature_step == \"PLS\":\n",
    "        steps = [\n",
    "            (\"mean_centering\", MeanCenterer()),\n",
    "            (\"feature_reduction\", PLSRegression_X(n_components=trial.suggest_categorical(\"PLS__n_components\", [2, 5, 10, 25, 50, 100]))),\n",
    "        ]\n",
    "    elif feature_step == \"PCA\":\n",
    "        steps = [\n",
    "            (\"mean_centering\", MeanCenterer()),\n",
    "            (\"feature_reduction\", PCA(n_components=trial.suggest_categorical(\"PCA__n_components\", [2, 5, 10, 25, 50, 100]), random_state=random_state)),\n",
    "        ]\n",
    "    elif feature_step == \"FeatureSelector\":\n",
    "        steps = [\n",
    "            (\"feature_reduction\", FeatureSelector(method=\"top_n_cv\", n_features=trial.suggest_categorical(\"FeatureSelector__n_features\", [250, 500, 1000, 5000, 17879]))),\n",
    "            (\"mean_centering\", MeanCenterer()),\n",
    "        ]\n",
    "    # Define model\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"SVR\", \"RFR\"])\n",
    "    if model_type == \"SVR\":\n",
    "        steps.append((\"model\", SVR(\n",
    "            kernel=trial.suggest_categorical(\"SVR__kernel\", [\"rbf\", \"poly\"]),\n",
    "            C=trial.suggest_float(\"SVR__C\", 1e-4, 1e2, log = True),\n",
    "            degree=trial.suggest_int(\"SVR__degree\", 2, 4),\n",
    "#             coef0=trial.suggest_uniform(\"SVR__coef0\", 0, 2),\n",
    "            gamma=0.001\n",
    "        )))\n",
    "    elif model_type == \"RFR\":\n",
    "        steps.append((\"model\", RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int(\"RFR__n_estimators\", 300, 1600, step=400),\n",
    "            max_features=trial.suggest_categorical(\"RFR__max_features\", [\"sqrt\", \"log2\", 0.5, 0.75, 1]),\n",
    "            max_samples=trial.suggest_categorical(\"RFR__max_samples\", [0.25, 0.5, 0.75, None]),\n",
    "            max_depth=trial.suggest_categorical(\"RFR__max_depth\", [None, 10, 25, 50, 100, 200]),\n",
    "            random_state=random_state,\n",
    "            n_jobs=int(n_cores/inner_cv.n_splits)\n",
    "        )))\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Evaluate with cross-validation\n",
    "    mse = -cross_val_score(pipeline, X, y, \n",
    "                           cv=inner_cv, \n",
    "                           scoring=\"neg_mean_squared_error\", \n",
    "                           n_jobs=inner_cv.n_splits).mean()\n",
    "\n",
    "#     for fold_idx, (train_idx, val_idx) in enumerate(inner_cv.split(X, y)):\n",
    "#         X_train, X_val = X[train_idx], X[val_idx]\n",
    "#         y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "#         # Train and evaluate the pipeline on the current fold\n",
    "#         pipeline.fit(X_train, y_train)\n",
    "#         y_val_pred = pipeline.predict(X_val)\n",
    "#         mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "#         # Store the MSE for this fold\n",
    "#         mse_scores.append(mse)\n",
    "\n",
    "#         # Report intermediate result to Optuna\n",
    "#         trial.report(np.mean(mse_scores), step=fold_idx)\n",
    "\n",
    "#         # Check if the trial should be pruned\n",
    "#         if trial.should_prune():\n",
    "#             raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "#     return np.mean(mse_scores)\n",
    "\n",
    "    return mse\n",
    "\n",
    "\n",
    "def generate_best_pipeline(study):\n",
    "    best_params = study.best_params\n",
    "    steps = []\n",
    "    if best_params[\"feature_step\"] == \"PLS\":\n",
    "        steps.append((\"mean_centering\", MeanCenterer()))\n",
    "        steps.append((\"feature_reduction\", PLSRegression_X(n_components=best_params[\"PLS__n_components\"])))\n",
    "    elif best_params[\"feature_step\"] == \"PCA\":\n",
    "        steps.append((\"mean_centering\", MeanCenterer()))\n",
    "        steps.append((\"feature_reduction\", PCA(n_components=best_params[\"PCA__n_components\"], random_state=random_state)))\n",
    "    elif best_params[\"feature_step\"] == \"FeatureSelector\":\n",
    "        steps.append((\"feature_reduction\", FeatureSelector(method=\"top_n_cv\", n_features=best_params[\"FeatureSelector__n_features\"])))\n",
    "        steps.append((\"mean_centering\", MeanCenterer()))\n",
    "\n",
    "    if \"SVR__kernel\" in best_params:\n",
    "        steps.append((\"model\", SVR(\n",
    "            kernel=best_params[\"SVR__kernel\"],\n",
    "            C=best_params[\"SVR__C\"],\n",
    "            degree=best_params[\"SVR__degree\"],\n",
    "#             coef0=best_params[\"SVR__coef0\"],\n",
    "            gamma=0.001\n",
    "        )))\n",
    "    elif \"RFR__n_estimators\" in best_params:\n",
    "        steps.append((\"model\", RandomForestRegressor(\n",
    "            n_estimators=best_params[\"RFR__n_estimators\"],\n",
    "            max_features=best_params[\"RFR__max_features\"],\n",
    "            max_samples=best_params[\"RFR__max_samples\"],\n",
    "            max_depth=best_params[\"RFR__max_depth\"],\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_cores\n",
    "        )))\n",
    "\n",
    "    best_pipeline = Pipeline(steps)\n",
    "    return best_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b0b152e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(os.path.join(data_path, 'processed',  'expr.csv'), index_col = 0).T.values\n",
    "y = pd.read_csv(os.path.join(data_path, 'processed', 'metastatic_potential.csv'), index_col = 0).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "23e5c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_folds=10\n",
    "inner_folds=5\n",
    "n_trials = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "08199bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 18:59:43,711] A new study created in memory with name: 0_optuna\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# cmaes_sampler = CmaEsSampler(seed=random_state, \n",
    "#                              warn_independent_sampling=False, \n",
    "#                             restart_strategy='bipop')\n",
    "\n",
    "# exploration_sampler = RandomSampler(seed=random_state)\n",
    "# tpe_sampler = RandomTPESampler(seed=random_state, \n",
    "#                                n_startup_trials = 25,\n",
    "#                                exploration_sampler = exploration_sampler, \n",
    "#                                exploration_freq=20)\n",
    "\n",
    "\n",
    "# def optuna_objective_toy(trial, X, y, inner_cv, n_cores, random_state):\n",
    "#     # Define feature reduction/selection method\n",
    "#     feature_step = trial.suggest_categorical(\"feature_step\", [\"PLS\", \"FeatureSelector\"])\n",
    "\n",
    "#     if feature_step == \"PLS\":\n",
    "#         steps = [\n",
    "#             (\"mean_centering\", MeanCenterer()),\n",
    "#             (\"feature_reduction\", PLSRegression_X(n_components=trial.suggest_categorical(\"PLS__n_components\", [2, 3]))),\n",
    "#         ]\n",
    "#     elif feature_step == \"FeatureSelector\":\n",
    "#         steps = [\n",
    "#             (\"feature_reduction\", FeatureSelector(method=\"top_n_cv\", n_features=trial.suggest_categorical(\"FeatureSelector__n_features\", [5,10]))),\n",
    "#             (\"mean_centering\", MeanCenterer()),\n",
    "#         ]\n",
    "#     # Define model\n",
    "#     model_type = trial.suggest_categorical(\"model_type\", [\"SVR\", \"RFR\"])\n",
    "#     if model_type == \"SVR\":\n",
    "#         steps.append((\"model\", SVR(\n",
    "#             kernel=trial.suggest_categorical(\"SVR__kernel\", [\"rbf\", \"poly\"]),\n",
    "#             gamma=0.001\n",
    "#         )))\n",
    "#     elif model_type == \"RFR\":\n",
    "#         steps.append((\"model\", RandomForestRegressor(\n",
    "#             n_estimators=trial.suggest_int(\"RFR__n_estimators\", 5, 7),\n",
    "#             random_state=random_state,\n",
    "#             n_jobs=int(n_cores/inner_cv.n_splits)\n",
    "#         )))\n",
    "\n",
    "#     # Create the pipeline\n",
    "#     pipeline = Pipeline(steps)\n",
    "\n",
    "#     # Evaluate with cross-validation\n",
    "#     mse = -cross_val_score(pipeline, X, y, \n",
    "#                            cv=inner_cv, \n",
    "#                            scoring=\"neg_mean_squared_error\", \n",
    "#                            n_jobs=inner_cv.n_splits).mean()\n",
    "\n",
    "#     return mse\n",
    "\n",
    "\n",
    "# def generate_best_pipeline_toy(study):\n",
    "#     best_params = study.best_params\n",
    "#     steps = []\n",
    "#     if best_params[\"feature_step\"] == \"PLS\":\n",
    "#         steps.append((\"mean_centering\", MeanCenterer()))\n",
    "#         steps.append((\"feature_reduction\", PLSRegression_X(n_components=best_params[\"PLS__n_components\"])))\n",
    "#     elif best_params[\"feature_step\"] == \"FeatureSelector\":\n",
    "#         steps.append((\"feature_reduction\", FeatureSelector(method=\"top_n_cv\", n_features=best_params[\"FeatureSelector__n_features\"])))\n",
    "#         steps.append((\"mean_centering\", MeanCenterer()))\n",
    "\n",
    "#     if \"SVR__kernel\" in best_params:\n",
    "#         steps.append((\"model\", SVR(\n",
    "#             kernel=best_params[\"SVR__kernel\"],\n",
    "#             gamma=0.001\n",
    "#         )))\n",
    "#     elif \"RFR__n_estimators\" in best_params:\n",
    "#         steps.append((\"model\", RandomForestRegressor(\n",
    "#             n_estimators=best_params[\"RFR__n_estimators\"],\n",
    "#             random_state=random_state,\n",
    "#             n_jobs=n_cores\n",
    "#         )))\n",
    "\n",
    "#     best_pipeline = Pipeline(steps)\n",
    "#     return best_pipeline\n",
    "\n",
    "# X = np.random.randn(20, 100)\n",
    "# y = np.random.randn(20,)\n",
    "# outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "# inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "# results = []\n",
    "# for k, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "#     print(str(k))\n",
    "#     X_train, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    \n",
    "#     pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "#     study = optuna.create_study(direction=\"minimize\", \n",
    "#                                 sampler=PeriodicHybridSampler(primary_sampler=cmaes_sampler, \n",
    "#                                                               fallback_sampler=tpe_sampler, \n",
    "#                                                              exploration_sampler=exploration_sampler, ), \n",
    "#                                pruner = pruner, \n",
    "#                                study_name = '{}_optuna'.format(k))\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5344f6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmaes_sampler = CmaEsSampler(seed=random_state, \n",
    "                             warn_independent_sampling=False, \n",
    "                            restart_strategy='bipop')\n",
    "\n",
    "exploration_sampler = RandomSampler(seed=random_state)\n",
    "tpe_sampler = RandomTPESampler(seed=random_state, \n",
    "                               n_startup_trials = 25,\n",
    "                               exploration_sampler = exploration_sampler, \n",
    "                               exploration_freq=20 # randomly sample every n trials\n",
    "                              )\n",
    "# tpe_sampler = TPESampler(seed=random_state, \n",
    "#                         n_startup_trials = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "results = []\n",
    "for k, (train_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "    print(str(k))\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    \n",
    "    pruner = optuna.pruners.SuccessiveHalvingPruner()\n",
    "    study = optuna.create_study(direction=\"minimize\", \n",
    "                                sampler=HybridSampler(primary_sampler=cmaes_sampler, fallback_sampler=tpe_sampler), \n",
    "                               pruner = pruner, \n",
    "                               study_name = '{}_optuna'.format(k))\n",
    "    study.optimize(\n",
    "        lambda trial: optuna_objective(trial, X_train, y_train, inner_cv, n_cores, random_state),\n",
    "        n_trials=n_trials, \n",
    "        catch=(ValueError,)\n",
    "    )\n",
    "    write_pickled_object(study, os.path.join(data_path, 'interim', study.study_name + '.pickle'))\n",
    "        \n",
    "    best_pipeline = generate_best_pipeline(study)\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    y_train_pred = best_pipeline.predict(X_train)\n",
    "    y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "    train_corr = pearsonr(y_train, y_train_pred)[0]\n",
    "    test_corr = pearsonr(y_test, y_test_pred)[0]\n",
    "\n",
    "    results.append({\n",
    "        \"fold\": k,\n",
    "        \"train_corr\": train_corr,\n",
    "        \"test_corr\": test_corr,\n",
    "        \"best_params\": study.best_params,\n",
    "        \"inner_cv\": study.trials_dataframe()\n",
    "        })\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df.to_csv(os.path.join(data_path, 'interim', 'pipeline.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "38e86a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(os.path.join(data_path, 'interim', 'pipeline.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2c1217fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature_step': 'PLS', 'PLS__n_components': 10, 'model_type': 'RFR', 'RFR__n_estimators': 300, 'RFR__max_features': 'sqrt', 'RFR__max_samples': None, 'RFR__max_depth': 200}\n",
      "------------------------------------------------------\n",
      "{'feature_step': 'PLS', 'PLS__n_components': 10, 'model_type': 'RFR', 'RFR__n_estimators': 300, 'RFR__max_features': 0.75, 'RFR__max_samples': 0.25, 'RFR__max_depth': 10}\n",
      "------------------------------------------------------\n",
      "{'feature_step': 'PLS', 'PLS__n_components': 25, 'model_type': 'RFR', 'RFR__n_estimators': 300, 'RFR__max_features': 'sqrt', 'RFR__max_samples': 0.25, 'RFR__max_depth': 10}\n",
      "------------------------------------------------------\n",
      "{'feature_step': 'PLS', 'PLS__n_components': 2, 'model_type': 'RFR', 'RFR__n_estimators': 1500, 'RFR__max_features': 'sqrt', 'RFR__max_samples': 0.25, 'RFR__max_depth': 200}\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for bp in res.best_params:\n",
    "    print(bp)\n",
    "    print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e03e1f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.32316588048141826)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.test_corr.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff38d20d",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb874ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = [\n",
    "#     (\"mean_centering\", MeanCenterer()),\n",
    "#     (\"feature_reduction\", PLSRegression_X(n_components=10)),\n",
    "# ]\n",
    "# steps.append((\"model\", RandomForestRegressor(\n",
    "#     n_estimators=300,\n",
    "#     max_features='sqrt',\n",
    "#     max_samples=None,\n",
    "#     max_depth=25,\n",
    "#     random_state=random_state,\n",
    "#     n_jobs=n_cores\n",
    "# )))\n",
    "# best_pipeline = Pipeline(steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3469a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# y_train_pred = best_pipeline.predict(X_train)\n",
    "# y_test_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "# train_corr = pearsonr(y_train, y_train_pred)[0]\n",
    "# test_corr = pearsonr(y_test, y_test_pred)[0]\n",
    "\n",
    "# y_test_pred = pd.cut(y_test_pred, bins=3, labels=['Low', 'Medium', 'High'])\n",
    "# y_test = pd.cut(y_test, bins = 3, labels=['Low', 'Medium', 'High'])\n",
    "# print(test_corr)\n",
    "# print(f1_score(y_test_pred, y_test, average='weighted'))\n",
    "# print((1/3) + (test_corr**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "86d9d66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "623e7404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(data_path, 'interim', study.study_name + '.pickle'), 'rb') as handle:\n",
    "#     study = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "20ce3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.sampler.primary_sampler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metastatic_potential]",
   "language": "python",
   "name": "conda-env-metastatic_potential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
