{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4c7cbc9",
   "metadata": {},
   "source": [
    "The top 25 features have no significant interactions. However, it is possible for interactions to occur non-intuitively across features that don't have high ranks. Here, we use a genetic algorithm to find a set of 25 features with strong interactions. \n",
    "\n",
    "We explore sets of features with high interactions amongst all, rather than sets of feature pairs for a couple reasons:\n",
    "1. By focusing on features, we capture recurring interactions between features. This is indicative of genes with multiple interactions, and could identify modules of interacting featuers. \n",
    "2. The combinatorial space of feature pairs (19638 choose 2 = 1.93e8) is much larger than that of features (19638)\n",
    "\n",
    "This script was run on 3 feature sets: all features (19638), those features that are present in the Cancer Cell Map, and those features that are present in the Cancer Gene Consensus. The latter two enable constraining of the search space to biologically relevant features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152080cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--feature_type\", type=str, required=True, help=\"how to filter features\")\n",
    "# args = parser.parse_args()\n",
    "# feature_type = args.feature_type\n",
    "\n",
    "# # python 05_perturbation_interactions_residuals_sets.py --feature_type all \n",
    "# # feature_type = 'all'\n",
    "feature_type = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7031b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/pool/005/hmbaghda/miniforge3/envs/metastatic_potential/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "from math import comb\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../../')\n",
    "from utils import read_pickled_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad78a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/hmbaghda/orcd/pool/metastatic_potential/'\n",
    "random_state = 42 + 3\n",
    "\n",
    "n_cores = 60\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(1)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(1)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(1)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(1)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fd369a",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "Same as [Notebook 05A](./05A_top_ranked_interactions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8f07970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/orcd/pool/005/hmbaghda/miniforge3/envs/metastatic_potential/lib/python3.11/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "/orcd/pool/005/hmbaghda/miniforge3/envs/metastatic_potential/lib/python3.11/site-packages/sklearn/pipeline.py:61: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "X = pd.read_csv(os.path.join(data_path, 'processed',  'expr_joint.csv'), index_col = 0)\n",
    "expr_joint = X.copy()\n",
    "\n",
    "mp_joint=pd.read_csv(os.path.join(data_path, 'processed', 'metastatic_potential_joint.csv'), index_col = 0)['mean']\n",
    "y = mp_joint.values.ravel()\n",
    "\n",
    "expr_protein = pd.read_csv(os.path.join(data_path, 'processed',  'expr_protein.csv'), index_col = 0)\n",
    "expr_rna = pd.read_csv(os.path.join(data_path, 'processed',  'expr.csv'), index_col = 0)\n",
    "\n",
    "protein_cols = expr_protein.columns\n",
    "rna_cols = expr_rna.columns\n",
    "\n",
    "X_protein = X[protein_cols].values\n",
    "X_rna = X[rna_cols].values\n",
    "\n",
    "# fit model\n",
    "best_pipeline = read_pickled_object(os.path.join(data_path, 'processed', \n",
    "                                                 'best_model_svr_linear_joint.pickle'))\n",
    "\n",
    "X = (X_protein, X_rna)\n",
    "best_pipeline = clone(best_pipeline)\n",
    "best_pipeline.fit(X, y)\n",
    "\n",
    "\n",
    "model_coefs = pd.read_csv(os.path.join(data_path, 'interim', 'joint_features.csv'), \n",
    "                          index_col = 0)\n",
    "if not np.allclose(model_coefs['SVM coefficient'].values, \n",
    "                   best_pipeline.named_steps['model'].coef_.flatten()):\n",
    "    raise ValueError('Inconsitency between Notebook 04 and 05')\n",
    "model_coefs.sort_values(by='SVM coefficient', key=lambda x: x.abs(), ascending=False, inplace=True)\n",
    "model_coefs.set_index('feature_name', inplace = True)\n",
    "\n",
    "# get prediction\n",
    "y_pred = best_pipeline.predict(X)\n",
    "residuals = y - y_pred\n",
    "\n",
    "# center\n",
    "X_map = {'Transcriptomics': X_rna, 'Proteomics': X_protein}\n",
    "X_map = {k: X_ - np.mean(X_, axis=0) for k, X_ in X_map.items()} # center the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fe2ba0",
   "metadata": {},
   "source": [
    "## GA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11d84fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_interaction_value_ga(feature_1: str, feature_2: str, \n",
    "                             residuals = residuals, \n",
    "                             X_map = X_map, \n",
    "                             model_coefs = model_coefs):\n",
    "    X_1 = X_map[model_coefs.loc[feature_1,'Modality']][:, model_coefs.loc[feature_1, 'feature_index']]\n",
    "    X_2 = X_map[model_coefs.loc[feature_2,'Modality']][:, model_coefs.loc[feature_2, 'feature_index']]\n",
    "\n",
    "    ols_df = pd.DataFrame({\"residual\": residuals, \n",
    "                           \"X_tilda\": X_1 * X_2})\n",
    "\n",
    "    ols_interaction = smf.ols(\"residual ~ X_tilda\", data=ols_df).fit()\n",
    "\n",
    "\n",
    "    coef = abs(ols_interaction.params.X_tilda)\n",
    "    se = ols_interaction.bse.X_tilda\n",
    "    \n",
    "    t_statistic = float(coef)/se\n",
    "    \n",
    "    return coef, t_statistic\n",
    "\n",
    "def generate_individual(seed, n_genes):\n",
    "    random.seed(seed)\n",
    "    return random.sample(all_features, n_genes)\n",
    "\n",
    "\n",
    "# --- Fitness function ---\n",
    "def evaluate_solution(feature_subset):\n",
    "    \"\"\"Iterates through all interactions in an individual.\"\"\"\n",
    "#     ALPHA=0.2\n",
    "    \n",
    "    pairs = itertools.combinations(feature_subset, 2)\n",
    "    coefs = []\n",
    "    t_stats = []\n",
    "    for f1, f2 in pairs:\n",
    "        coef, t_statistic = get_interaction_value_ga(f1, f2)\n",
    "        coefs.append(coef)\n",
    "        t_stats.append(t_statistic)\n",
    "    \n",
    "    # log-transform\n",
    "    coefs = np.log1p(np.array(coefs))\n",
    "    t_stats = np.log1p(np.array(t_stats))\n",
    "\n",
    "#     # log-transform due to extreme range of coefs\n",
    "#     coef_arr = np.log1p(np.array(coefs)) \n",
    "#     tstat_arr = np.log1p(np.array(t_stats))\n",
    "    \n",
    "#     # Min-max normalize both arrays -- not doing because makes changes too sublte\n",
    "#     # Avoid divide-by-zero with epsilon\n",
    "#     eps = 1e-8\n",
    "#     coef_norm = (coef_arr - coef_arr.min()) / (coef_arr.max() - coef_arr.min() + eps)\n",
    "#     tstat_norm = (tstat_arr - tstat_arr.min()) / (tstat_arr.max() - tstat_arr.min() + eps)\n",
    "\n",
    "    # Combine with weight alpha\n",
    "#     hybrid_scores = ALPHA * coef_norm + (1 - ALPHA) * tstat_norm\n",
    "\n",
    "    return coefs, t_stats\n",
    "\n",
    "def rank_selection(seed, pop, scores, num_selected):\n",
    "    '''Assign probability of selection according to rank order of the population'''\n",
    "    \n",
    "    # rank order by lowest fitness to highest fitness\n",
    "    sorted_pop = [x for _, x in sorted(zip(scores, pop), key=lambda x: x[0])] \n",
    "    ranks = list(range(1, len(sorted_pop) + 1))\n",
    "    probs = [r / sum(ranks) for r in ranks]\n",
    "    \n",
    "    # select according to probability of rank\n",
    "    random.seed(seed)\n",
    "    selected_pop = random.choices(sorted_pop, weights=probs, k=num_selected)\n",
    "    return selected_pop\n",
    "\n",
    "\n",
    "def score_fitness(raw_fitness_components, lambda_ = 0.25):\n",
    "    '''Scores fitness of each individual in population for a given generation.\n",
    "    \n",
    "    \n",
    "    Evaluates by a hybrid scoring of t-statistic and abs coefficient. \n",
    "    \n",
    "    These components are log-transformed in evaluate_solution.\n",
    "    \n",
    "    T-statistic controls for the SE. Since t-statistic is normalized and coefficient is not, \n",
    "    we min-max scale both so that they are on similar scales. \n",
    "    \n",
    "    lambda_ controls weighting of coef vs t-statistic on score. \n",
    "    '''\n",
    "    # min-max scale individuals according to population\n",
    "    population_coefs = [c for coefs, _ in raw_fitness_components for c in coefs]\n",
    "    population_tstats = [t for _, tstats in raw_fitness_components for t in tstats]\n",
    "\n",
    "    coef_min, coef_max = np.min(population_coefs), np.max(population_coefs)\n",
    "    tstat_min, tstat_max = np.min(population_tstats), np.max(population_tstats)\n",
    "\n",
    "    eps = 1e-12 # avoid dividing by 0\n",
    "\n",
    "    fitness_scores = []\n",
    "    for coefs, tstats in raw_fitness_components:\n",
    "        # scale individuals \n",
    "        coef_arr = (np.array(coefs) - coef_min) / (coef_max - coef_min + eps)\n",
    "        tstat_arr = (np.array(tstats) - tstat_min) / (tstat_max - tstat_min + eps)\n",
    "\n",
    "        # Compute hybrid score\n",
    "        hybrid_scores = lambda_ * coef_arr + (1 - lambda_) * tstat_arr\n",
    "        fitness_scores.append(float(np.median(hybrid_scores)))\n",
    "        \n",
    "    return fitness_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "babbb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeedTracker:\n",
    "    def __init__(self):\n",
    "        self._gi_seed = 42\n",
    "        self._rs_seed = 888\n",
    "        self._offspring_1 = 2024\n",
    "        self._offspring_2a = 4048\n",
    "        self._offspring_2b = 64768\n",
    "        self._offspring_3 = 8096\n",
    "        self._mutation_seed_1 = 16192\n",
    "        self._mutation_seed_2 = 32384\n",
    "\n",
    "    @property\n",
    "    def gi_seed(self):\n",
    "        self._gi_seed += 1\n",
    "        return self._gi_seed\n",
    "    \n",
    "    @property\n",
    "    def rs_seed(self):\n",
    "        self._rs_seed += 1\n",
    "        return self._rs_seed\n",
    "    \n",
    "    @property\n",
    "    def offspring_1(self):\n",
    "        self._offspring_1 += 1\n",
    "        return self._offspring_1\n",
    "    \n",
    "    @property\n",
    "    def offspring_2a(self):\n",
    "        self._offspring_2a += 1\n",
    "        return self._offspring_2a\n",
    "    \n",
    "    @property\n",
    "    def offspring_2b(self):\n",
    "        self._offspring_2b += 1\n",
    "        return self._offspring_2b\n",
    "\n",
    "    @property\n",
    "    def offspring_3(self):\n",
    "        self._offspring_3 += 1\n",
    "        return self._offspring_3\n",
    "    \n",
    "    @property\n",
    "    def mutation_seed_1(self):\n",
    "        self._mutation_seed_1 += 1\n",
    "        return self._mutation_seed_1\n",
    "\n",
    "    @property\n",
    "    def mutation_seed_2(self):\n",
    "        self._mutation_seed_2 += 1\n",
    "        return self._mutation_seed_2\n",
    "    \n",
    "seed_tracker = SeedTracker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e98b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'v3' # v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b4c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parameters ---\n",
    "if fn == 'v2':\n",
    "    POP_SIZE = 80*9 # no. of solutions\n",
    "    n_cores = min(n_cores, POP_SIZE)\n",
    "    N_GENES = 25 # feature set per solution (find n highly interacting genes)\n",
    "    N_GENERATIONS = 300 # no. of iterations of GA\n",
    "    #MUTATION_RATE = 0.2\n",
    "    GENE_MUTATION_RATE = 0.1 # probability of mutating each gene in each child\n",
    "\n",
    "\n",
    "    # injecting some randomness and monitoring convergence\n",
    "    patience = min(15, np.round(N_GENERATIONS / 10)) # early stopping after plateau for n iterations\n",
    "    n_restart_individuals = max(int(np.round(POP_SIZE*0.15)), 2) # randomly replace the bottom 15% of the population \n",
    "    restart_n_iterations = int(np.round(N_GENERATIONS)*0.2) # randomly replace every n generations\n",
    "\n",
    "    n_restarts_prior_to_break = 3 # allows n restarted prior to breaking\n",
    "    mutation_increase = 0.05\n",
    "    mgnr = GENE_MUTATION_RATE + n_restarts_prior_to_break*mutation_increase\n",
    "\n",
    "\n",
    "############\n",
    "if fn == 'v3':\n",
    "    POP_SIZE = 80*9 # no. of solutions\n",
    "    n_cores = min(n_cores, POP_SIZE)\n",
    "    N_GENES = 25 # feature set per solution (find n highly interacting genes)\n",
    "    N_GENERATIONS = 600 # no. of iterations of GA\n",
    "    GENE_MUTATION_RATE = 0.1 # probability of mutating each gene in each child\n",
    "\n",
    "\n",
    "    # injecting some randomness and monitoring convergence\n",
    "    patience = 40 #min(, np.round(N_GENERATIONS / 10)) # early stopping after plateau for n iterations\n",
    "    n_restart_individuals = max(int(np.round(POP_SIZE*0.15)), 2) # randomly replace the bottom 15% of the population \n",
    "    restart_n_iterations = int(np.round(N_GENERATIONS)*0.2) # randomly replace every n generations\n",
    "\n",
    "    n_restarts_prior_to_break = 3 # allows n restarted prior to breaking\n",
    "    mutation_increase = 0.05\n",
    "    mgnr = GENE_MUTATION_RATE + n_restarts_prior_to_break*mutation_increase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6aea8",
   "metadata": {},
   "source": [
    "Define a subset of features to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39820f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if feature_type == 'all':\n",
    "    all_features = model_coefs.index.tolist()\n",
    "    fn += ''\n",
    "elif feature_type == 'top400':\n",
    "    top_n = 400\n",
    "    all_features = model_coefs.index.tolist()[:top_n]\n",
    "    fn += 'top{}'.format(top_n)\n",
    "elif feature_type == 'enriched':\n",
    "    for key in ['negative', 'positive']:\n",
    "        enriched_genes = set()\n",
    "        ms = pd.read_excel(os.path.join(data_path, 'processed', key + '_joint_metascape_results.xlsx'), \n",
    "                           sheet_name = 'Enrichment',\n",
    "                           index_col = None)\n",
    "        enriched_genes = enriched_genes.union(set.union(*ms.Symbols.apply(lambda x: set(x.split(','))).tolist()))\n",
    "    all_features = model_coefs[model_coefs.gene_name.isin(enriched_genes)].index.tolist()\n",
    "    fn += 'enriched'\n",
    "elif feature_type == 'transcription_factors':\n",
    "    # from scLEMBAS, load the static collectri network as of June 2024\n",
    "    grn_link = 'https://zenodo.org/records/11477837/files/grn_organism_06_04_24.csv'\n",
    "    grn = 'collectri'\n",
    "    organism = 'human'\n",
    "    net = pd.read_csv(grn_link.replace('grn', grn).replace('organism', organism), index_col = 0)\n",
    "    tfs = net.source.unique().tolist()\n",
    "    all_features = model_coefs[model_coefs.gene_name.isin(tfs)].index.tolist()\n",
    "    fn += 'tfs'\n",
    "elif feature_type == 'cancer_cell_map':\n",
    "    # from scLEMBAS, load the statist omnipath DB and parse the cancer cell map\n",
    "    ppi_link = 'https://zenodo.org/records/11477837/files/organism_omnipath_ppi_05_24_24.csv'\n",
    "    organism = 'human'\n",
    "    sn_ppis = pd.read_csv(ppi_link.replace('organism', organism), index_col = 0) \n",
    "\n",
    "    sn_ppis.sources = sn_ppis.sources.apply(lambda x: x.split(';'))\n",
    "    ccm = sn_ppis[sn_ppis.sources.apply(lambda x: 'CancerCellMap' in x)]\n",
    "\n",
    "    ccm_genes = sorted(set(ccm.source_genesymbol.tolist() + ccm.target_genesymbol.tolist()))\n",
    "    all_features = model_coefs[model_coefs.gene_name.isin(ccm_genes)].index.tolist()\n",
    "    fn += 'ccm'\n",
    "elif feature_type == 'cancer_gene_consensus':\n",
    "    cgc = pd.read_csv(os.path.join(data_path, 'raw', 'Cosmic_CancerGeneCensus_v101_GRCh38_04_07_25.tsv'), \n",
    "                  sep = '\\t')\n",
    "    cgc = cgc.GENE_SYMBOL.unique().tolist()\n",
    "    all_features = model_coefs[model_coefs.gene_name.isin(cgc)].index.tolist()\n",
    "    fn += 'cgc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d661d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tracker = pd.DataFrame(columns = range(POP_SIZE))\n",
    "\n",
    "population = [generate_individual(seed_tracker.gi_seed, N_GENES) for _ in range(POP_SIZE)] # initialize solutions\n",
    "best_so_far = -np.inf\n",
    "best_set = []\n",
    "broken = False\n",
    "no_improvement = 0\n",
    "\n",
    "for gen in range(N_GENERATIONS):\n",
    "    print(f\"Generation {gen + 1}/{N_GENERATIONS}\")\n",
    "    \n",
    "    # get log-transformed coefs/tstats for each population\n",
    "    if n_cores in [1, 0, None]:\n",
    "        raw_fitness_components = [evaluate_solution(ind) for ind in tqdm(population)] \n",
    "    else:\n",
    "        with Pool(n_cores) as pool:\n",
    "            raw_fitness_components = list(tqdm(pool.imap(evaluate_solution, population), total=len(population)))\n",
    "\n",
    "    # score with hybrid of t-stat and coefficient\n",
    "    # scale values across population\n",
    "    fitness_scores = score_fitness(raw_fitness_components, lambda_ = 0.25)\n",
    "\n",
    "            \n",
    "    # track # of iterations that have no improvement in best fitness score\n",
    "    current_best = max(fitness_scores)\n",
    "    if current_best > best_so_far:\n",
    "        best_so_far = current_best\n",
    "        best_set = population[np.argmax(fitness_scores)]\n",
    "        no_improvement = 0\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        \n",
    "    if no_improvement >= patience:\n",
    "        if GENE_MUTATION_RATE < mgnr: # diversify \n",
    "            # increase gene mutation rate\n",
    "            GENE_MUTATION_RATE += mutation_increase\n",
    "            \n",
    "            # randomly replace bottom percent of individuals \n",
    "            sorted_population = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "            population = sorted_population[:-n_restart_individuals] + [generate_individual(seed_tracker.gi_seed, N_GENES) for _ in range(n_restart_individuals)]\n",
    "            no_improvement = 0 # reset \n",
    "        else: # early stopping\n",
    "            broken = True\n",
    "            break\n",
    "            \n",
    "    # randomly replace bottom percent of individuals every n generations\n",
    "    if gen % restart_n_iterations == 0 and gen != 0:\n",
    "        # randomly replace a subet of the population\n",
    "        sorted_population = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "        population = sorted_population[:-n_restart_individuals] + [generate_individual(seed_tracker.gi_seed, N_GENES) for _ in range(n_restart_individuals)]\n",
    "        \n",
    "    \n",
    "    score_tracker.loc[gen, :] = fitness_scores\n",
    "    \n",
    "#     # Selection: keep top 50% (elitism)\n",
    "#     sorted_pop = [x for _, x in sorted(zip(fitness_scores, population), key=lambda pair: pair[0], reverse=True)]\n",
    "#     population = sorted_pop[:POP_SIZE // 2]\n",
    "\n",
    "    # Selection: Rank selection\n",
    "    population = rank_selection(seed = seed_tracker.rs_seed, \n",
    "                                pop = population, \n",
    "                                scores = fitness_scores, \n",
    "                                num_selected = POP_SIZE //2)\n",
    "\n",
    "    # Reproduction: crossover\n",
    "    offspring = []\n",
    "    while len(offspring) < POP_SIZE - len(population):\n",
    "        random.seed(seed_tracker.offspring_1)\n",
    "        p1, p2 = random.sample(population, 2)\n",
    "\n",
    "        if gen < N_GENERATIONS // 3: # multiple cut points early in evolution (first 30%)\n",
    "            random.seed(seed_tracker.offspring_2a)\n",
    "            n_cuts = random.randint(2, 4)  # choose 2 to 4 cut points\n",
    "            cut_points = sorted(random.sample(range(1, N_GENES - 1), n_cuts))\n",
    "            cut_points = [0] + cut_points + [N_GENES]\n",
    "\n",
    "            # Alternate segments from parents\n",
    "            child = []\n",
    "            for i in range(len(cut_points) - 1):\n",
    "                start, end = cut_points[i], cut_points[i + 1]\n",
    "                segment = p1[start:end] if i % 2 == 0 else p2[start:end]\n",
    "                child.extend(segment)\n",
    "            child = list(dict.fromkeys(child))\n",
    "        else: # one cut point later in evolution\n",
    "            min_cut = int(N_GENES * 0.2)\n",
    "            max_cut = int(N_GENES * 0.8)\n",
    "            random.seed(seed_tracker.offspring_2b)\n",
    "            cut = random.randint(min_cut, max_cut) # random.randint(1, N_GENES - 2)\n",
    "            child = list(dict.fromkeys(p1[:cut] + p2[cut:]))  # remove duplicates while preserving order\n",
    "\n",
    "        # If child has < N_GENES genes, fill with random unused features\n",
    "        if len(child) < N_GENES:\n",
    "            unused = list(set(all_features) - set(child))\n",
    "            random.seed(seed_tracker.offspring_3)\n",
    "            child += random.sample(unused, N_GENES - len(child))\n",
    "\n",
    "        offspring.append(child)\n",
    "\n",
    "        \n",
    "    # MUTATION\n",
    "#     for child in offspring:\n",
    "#         for i in range(N_GENES):\n",
    "#             random.seed(seed_tracker.mutation_seed_1)\n",
    "#             if random.random() < GENE_MUTATION_RATE:\n",
    "#                 # Replace current gene with a random, unused gene\n",
    "#                 available_genes = list(set(all_features) - set(child))\n",
    "#                 if available_genes:  # make sure there's something to choose from\n",
    "#                     random.seed(seed_tracker.mutation_seed_2)\n",
    "#                     new_gene = random.choice(available_genes)\n",
    "#                     child[i] = new_gene\n",
    "    for child in tqdm(offspring): # faster version\n",
    "        current_set = set(child)\n",
    "        available_genes = list(set(all_features) - current_set)\n",
    "\n",
    "        if not available_genes:\n",
    "            continue  # nothing to mutate into\n",
    "\n",
    "        for i in range(N_GENES):\n",
    "            random.seed(seed_tracker.mutation_seed_1)\n",
    "            if random.random() < GENE_MUTATION_RATE:\n",
    "                new_gene = random.choice(available_genes)\n",
    "                available_genes.remove(new_gene)   # avoid reuse\n",
    "                current_set.remove(child[i])\n",
    "                current_set.add(new_gene)\n",
    "                child[i] = new_gene\n",
    "\n",
    "    # Form new population\n",
    "    population += offspring\n",
    "    if len(population) != POP_SIZE:\n",
    "        raise ValueError('Something went wrong here')\n",
    "        \n",
    "        \n",
    "    if (gen % 10 == 0) and gen != 0:\n",
    "        score_tracker.to_csv(os.path.join(data_path, 'processed', 'joint_interaction_residuals_ga_scores_sets' + fn + '.csv'))\n",
    "\n",
    "        with open(os.path.join(data_path, 'interim', 'joint_interaction_residuals_ga_solution_sets' + fn + '.txt'), \"w\") as f:\n",
    "            for item in best_set:\n",
    "                f.write(f\"{item}\\n\")\n",
    "\n",
    "# --- Final evaluation ---\n",
    "if not broken:\n",
    "    final_scores = [evaluate_solution(ind) for ind in tqdm(population)]\n",
    "    score_tracker.loc[score_tracker.shape[0], :] = final_scores\n",
    "    \n",
    "score_tracker.to_csv(os.path.join(data_path, 'processed', 'joint_interaction_residuals_ga_scores_sets' + fn + '.csv'))\n",
    "\n",
    "with open(os.path.join(data_path, 'interim', 'joint_interaction_residuals_ga_solution_sets' + fn + '.txt'), \"w\") as f:\n",
    "    for item in best_set:\n",
    "        f.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00de6fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metastatic_potential]",
   "language": "python",
   "name": "conda-env-metastatic_potential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
