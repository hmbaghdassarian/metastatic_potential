{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c6c14f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "from utils import get_stats, read_pickled_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4ba540",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/nobackup/users/hmbaghda/metastatic_potential/'\n",
    "random_state = 42 + 4\n",
    "\n",
    "n_cores = 30\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = str(n_cores)\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = str(n_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b62ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pipeline = read_pickled_object(os.path.join(data_path, 'processed', 'best_model.pickle'))\n",
    "\n",
    "X = pd.read_csv(os.path.join(data_path, 'processed',  'expr.csv'), index_col = 0).values\n",
    "y = pd.read_csv(os.path.join(data_path, 'processed', 'metastatic_potential.csv'), index_col = 0)['mean'].values.ravel()\n",
    "\n",
    "# this is only available after running script 01 in notebook C\n",
    "y_joint = pd.read_csv(os.path.join(data_path, 'processed',  'metastatic_potential_joint.csv'), index_col = 0)\n",
    "y_proteomics = pd.read_csv(os.path.join(data_path, 'processed',  'metastatic_potential_protein.csv'), index_col = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0806189",
   "metadata": {},
   "source": [
    "When iterating through sample sizes, we will keep the test dataset size the same (maximum amount -- 20% of the full dataset). This is to ensure metric stability. \n",
    "\n",
    "We also subset N times per sample size to ensure stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83cffc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up manual fold params\n",
    "n_splits = 10\n",
    "\n",
    "fold_size = X.shape[0] // n_splits\n",
    "train_samples_per_fold = int(np.round(X.shape[0] * ((100 - n_splits)/100)))\n",
    "\n",
    "# prepare sample sizes\n",
    "sample_sizes = np.arange(0.1,1.1, 0.1)\n",
    "\n",
    "n_samples_iter = [int(np.round(train_samples_per_fold*sample_size)) for sample_size in sample_sizes]\n",
    "if y_joint.shape[0] not in n_samples_iter:\n",
    "    n_samples_iter.append(y_joint.shape[0]) # ensure the actualy join omics sample size is included\n",
    "    n_samples_iter = sorted(n_samples_iter)\n",
    "    \n",
    "if y_proteomics.shape[0] not in n_samples_iter:\n",
    "    n_samples_iter.append(y_proteomics.shape[0]) # ensure the actualy join omics sample size is included\n",
    "    n_samples_iter = sorted(n_samples_iter)\n",
    "\n",
    "# subsets per sample size\n",
    "n_subsets = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ecd81c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# res = pd.DataFrame(columns = ['fold', 'train_sample_size', \n",
    "#                              'train_corr', 'test_corr', 'train_mse', 'test_mse'])\n",
    "# splits = {}\n",
    "\n",
    "\n",
    "# np.random.seed(random_state)\n",
    "# for k in range(n_splits):\n",
    "#     print(k)\n",
    "#     # manually get splits in order to ensure the are the same size across all folds\n",
    "#     shuffled_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "#     train_idx = shuffled_index[:train_samples_per_fold]\n",
    "#     test_idx = shuffled_index[train_samples_per_fold:]\n",
    "    \n",
    "#     X_train_full, X_test = X[train_idx], X[test_idx]\n",
    "#     y_train_full, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "#     splits[k] = {'test_idx': test_idx.tolist()}\n",
    "    \n",
    "#     for n_samples in n_samples_iter:\n",
    "#         splits[k]['train_' + str(n_samples)] = {}\n",
    "#         if n_samples < train_samples_per_fold:\n",
    "#             for i in range(n_subsets):\n",
    "#                 subset_idx = np.random.choice(train_samples_per_fold, n_samples, replace=False)\n",
    "#                 X_train = X_train_full[subset_idx, :]\n",
    "#                 y_train = y_train_full[subset_idx]\n",
    "\n",
    "#                 train_corr, test_corr, train_mse, test_mse = get_stats(best_pipeline, y_train, y_test, X_train, X_test)\n",
    "\n",
    "#                 res.loc[res.shape[0],:] = [k, n_samples, train_corr, test_corr, train_mse, test_mse]\n",
    "#                 splits[k]['train_' + str(n_samples)]['iter_' + str(i)] = subset_idx.tolist()\n",
    "#         else:\n",
    "#             train_corr, test_corr, train_mse, test_mse = get_stats(best_pipeline, y_train_full, y_test, X_train_full, X_test)\n",
    "#             res.loc[res.shape[0],:] = [k, n_samples, train_corr, test_corr, train_mse, test_mse]\n",
    "#             splits[k]['train_' + str(n_samples)]['full'] = train_idx.tolist()\n",
    "            \n",
    "#     res.to_csv(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics_linear_svr.csv'))\n",
    "            \n",
    "            \n",
    "# for k, inner_dict in splits.items():\n",
    "#     inner_dict_ = {}\n",
    "#     for k_, v in inner_dict.items():\n",
    "#         bool_a = k_ == 'test_idx'\n",
    "#         bool_b = k_.split('_')[1].isdigit() and int(k_.split('_')[1]) not in [y_joint.shape[0], y_proteomics.shape[0]]\n",
    "#         if bool_a or bool_b:\n",
    "#             inner_dict_[k] = v\n",
    "#     splits[k] = inner_dict_\n",
    "    \n",
    "# with open(os.path.join(data_path, 'interim', 'transcriptomics_power_analysis_folds.json'), \"w\") as json_file:\n",
    "#     json.dump(splits, json_file, indent=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8e434f",
   "metadata": {},
   "source": [
    "Add top-performing non-linear models to power analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3ce6265",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'interim', 'transcriptomics_power_analysis_folds.json'), \"r\") as json_file:\n",
    "    splits = json.load(json_file)\n",
    "    \n",
    "    \n",
    "best_pipelines = {\n",
    "    'svr_rbf': read_pickled_object(os.path.join(data_path, 'processed', \n",
    "                                 'best_model_svr_rbf_transcriptomics.pickle')), \n",
    "    'svr_poly': read_pickled_object(os.path.join(data_path, 'processed', \n",
    "                                 'best_model_svr_poly_transcriptomics.pickle'))\n",
    "}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e82fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics.csv')):\n",
    "    res = pd.read_csv(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics.csv'), \n",
    "                     index_col = 0)\n",
    "else:\n",
    "    res = pd.read_csv(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics_linear_svr.csv'), \n",
    "                     index_col = 0)\n",
    "    res['model'] = 'svr_linear'\n",
    "    res = res[['model', 'fold', 'train_sample_size', \n",
    "                                 'train_corr', 'test_corr', 'train_mse', 'test_mse']]\n",
    "\n",
    "\n",
    "for model_name, best_pipeline in best_pipelines.items():\n",
    "    for k in range(n_splits):\n",
    "        if res[(res.model == model_name) & (res.fold == k)].shape[0] == 0:\n",
    "            print('Model: ' + model_name + ', fold: ' + str(k))\n",
    "            train_idx = splits[k]['train_433']['full']\n",
    "            test_idx = splits[k]['test_idx']\n",
    "\n",
    "            X_train_full, X_test = X[train_idx], X[test_idx]\n",
    "            y_train_full, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            for (j, n_samples) in enumerate(n_samples_iter):\n",
    "                bool_a = n_samples not in [y_joint.shape[0], y_proteomics.shape[0]] \n",
    "#                 bool_b = (model_name == 'svr_linear')\n",
    "                if n_samples < train_samples_per_fold:\n",
    "                    if bool_a:# or bool_b:\n",
    "                        for i in range(n_subsets):\n",
    "                            np.random.seed(random_state + k + i + (j*n_subsets))\n",
    "                            subset_idx = splits[k]['train_' + str(n_samples)]['iter_' + str(i)]\n",
    "                            X_train = X_train_full[subset_idx, :]\n",
    "                            y_train = y_train_full[subset_idx]\n",
    "\n",
    "                            train_corr, test_corr, train_mse, test_mse = get_stats(best_pipeline, y_train, y_test, X_train, X_test)\n",
    "\n",
    "                            res.loc[res.shape[0],:] = [model_name, k, n_samples, train_corr, test_corr, train_mse, test_mse]\n",
    "                else:\n",
    "                    train_corr, test_corr, train_mse, test_mse = get_stats(best_pipeline, y_train_full, y_test, X_train_full, X_test)\n",
    "                    res.loc[res.shape[0],:] = [model_name, k, n_samples, train_corr, test_corr, train_mse, test_mse]\n",
    "\n",
    "            res.to_csv(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d62d67",
   "metadata": {},
   "source": [
    "# Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0c3845f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv(os.path.join(data_path, 'processed', 'power_analysis_transcriptomics.csv'), index_col = 0)\n",
    "\n",
    "\n",
    "# just visualize the equal interval samples\n",
    "res = res[~res.train_sample_size.isin([y_joint.shape[0], y_proteomics.shape[0]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa20cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_types = ['corr', 'mse']\n",
    "mmap = dict(zip(metric_types, ['Pearson Correlation', 'MSE']))\n",
    "title_map = {'svr_linear': 'Linear SVR', \n",
    "            'svr_rbf': 'RBF SVR', \n",
    "            'svr_poly': 'Polynomial SVR'}\n",
    "\n",
    "\n",
    "ncols = res.model.nunique()\n",
    "nrows = len(metric_types)\n",
    "fig, axes = plt.subplots(ncols = ncols, nrows = nrows, figsize = (5.5*ncols, 4.7*nrows))\n",
    "\n",
    "for (j, model_name) in enumerate(res.model.unique()):\n",
    "    viz_df = res[res.model == model_name]\n",
    "    \n",
    "    for (i, metric_type) in enumerate(metric_types):\n",
    "        sns.boxplot(data = viz_df, x = 'train_sample_size', y = 'test_' + metric_type, \n",
    "                   ax = axes[i,j])\n",
    "        axes[i,j].set_title(title_map[model_name])\n",
    "        axes[i,j].set_ylabel(mmap[metric_type])\n",
    "        axes[i,j].set_xlabel('Training Sample Size')\n",
    "        \n",
    "        \n",
    "        \n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metastatic_potential]",
   "language": "python",
   "name": "conda-env-metastatic_potential-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
